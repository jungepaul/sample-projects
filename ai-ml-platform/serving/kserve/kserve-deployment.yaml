apiVersion: v1
kind: Namespace
metadata:
  name: kserve

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: inferenceservice-config
  namespace: kserve
data:
  inferenceservice: |
    {
      "storageSpec": {
        "storageSpecsVersion": "v1alpha1"
      },
      "predictors": {
        "tensorflow": {
          "image": "tensorflow/serving",
          "defaultImageVersion": "2.6.2",
          "defaultGpuImageVersion": "2.6.2-gpu",
          "defaultTimeout": "60",
          "supportedFrameworks": [
            "tensorflow"
          ],
          "multiModelServer": false
        },
        "pytorch": {
          "image": "pytorch/torchserve",
          "defaultImageVersion": "0.7.1-cpu",
          "defaultGpuImageVersion": "0.7.1-gpu", 
          "defaultTimeout": "60",
          "supportedFrameworks": [
            "pytorch"
          ],
          "multiModelServer": false
        },
        "sklearn": {
          "image": "kserve/sklearnserver",
          "defaultImageVersion": "v0.11.0",
          "supportedFrameworks": [
            "sklearn"
          ],
          "multiModelServer": true
        },
        "xgboost": {
          "image": "kserve/xgbserver",
          "defaultImageVersion": "v0.11.0",
          "supportedFrameworks": [
            "xgboost"
          ],
          "multiModelServer": true
        },
        "pmml": {
          "image": "kserve/pmmlserver",
          "defaultImageVersion": "v0.11.0",
          "supportedFrameworks": [
            "pmml"
          ],
          "multiModelServer": false
        },
        "lightgbm": {
          "image": "kserve/lgbserver",
          "defaultImageVersion": "v0.11.0",
          "supportedFrameworks": [
            "lightgbm"
          ],
          "multiModelServer": false
        },
        "mlflow": {
          "image": "kserve/mlflowserver",
          "defaultImageVersion": "v0.11.0",
          "supportedFrameworks": [
            "mlflow"
          ],
          "multiModelServer": false
        }
      },
      "transformers": {
        "feast": {
          "image": "kserve/feast-transformer",
          "defaultImageVersion": "v0.11.0"
        }
      },
      "explainers": {
        "alibi": {
          "image": "kserve/alibi-explainer",
          "defaultImageVersion": "v0.11.0"
        },
        "aix": {
          "image": "kserve/aix-explainer", 
          "defaultImageVersion": "v0.11.0"
        },
        "art": {
          "image": "kserve/art-explainer",
          "defaultImageVersion": "v0.11.0"
        }
      },
      "storageInitializer": {
        "image": "kserve/storage-initializer:v0.11.0",
        "memoryRequest": "100Mi",
        "memoryLimit": "1Gi",
        "cpuRequest": "100m",
        "cpuLimit": "1"
      },
      "credentials": {
        "gcs": {
          "gcsCredentialFileName": "gcloud-application-credentials.json"
        },
        "s3": {
          "s3AccessKeyIDName": "AWS_ACCESS_KEY_ID",
          "s3SecretAccessKeyName": "AWS_SECRET_ACCESS_KEY",
          "s3Endpoint": "",
          "s3UseHttps": "",
          "s3Region": "",
          "s3VerifySSL": "",
          "s3UseVirtualBucket": "",
          "s3UseAnonymousCredential": "",
          "s3CABundle": ""
        }
      },
      "ingress": {
        "ingressGateway": "kserve/kserve-gateway",
        "ingressService": "istio-ingressgateway.istio-system.svc.cluster.local",
        "localGateway": "kserve/kserve-local-gateway",
        "localGatewayService": "kserve-local-gateway.istio-system.svc.cluster.local",
        "ingressDomain": "example.com",
        "ingressClassName": "istio",
        "domainTemplate": "{{.Name}}-{{.Namespace}}.{{.IngressDomain}}",
        "urlScheme": "http",
        "disableIstioVirtualHost": false
      },
      "logger": {
        "image": "kserve/agent:v0.11.0",
        "memoryRequest": "100Mi",
        "memoryLimit": "1Gi",
        "cpuRequest": "100m",
        "cpuLimit": "1",
        "defaultUrl": "http://default-broker"
      },
      "batcher": {
        "image": "kserve/agent:v0.11.0",
        "memoryRequest": "1Gi",
        "memoryLimit": "2Gi", 
        "cpuRequest": "1",
        "cpuLimit": "2"
      },
      "agent": {
        "image": "kserve/agent:v0.11.0",
        "memoryRequest": "100Mi",
        "memoryLimit": "1Gi",
        "cpuRequest": "100m",
        "cpuLimit": "1"
      },
      "router": {
        "image": "kserve/router:v0.11.0",
        "memoryRequest": "100Mi",
        "memoryLimit": "1Gi",
        "cpuRequest": "100m",
        "cpuLimit": "1"
      }
    }

---
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: kserve-gateway
  namespace: kserve
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "*.kserve.local"

---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: sklearn-iris
  namespace: kserve
spec:
  predictor:
    sklearn:
      storageUri: "gs://kfserving-examples/models/sklearn/1.0/model"
      resources:
        requests:
          cpu: "500m"
          memory: "512Mi"
        limits:
          cpu: "1000m"
          memory: "1Gi"

---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: tensorflow-flowers
  namespace: kserve
spec:
  predictor:
    tensorflow:
      storageUri: "gs://kfserving-examples/models/tensorflow/flowers"
      resources:
        requests:
          cpu: "1000m"
          memory: "1Gi"
        limits:
          cpu: "2000m"
          memory: "2Gi"

---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: mlflow-wine-classifier
  namespace: kserve
  annotations:
    serving.kserve.io/autoscaler-class: "hpa"
spec:
  predictor:
    minReplicas: 1
    maxReplicas: 5
    scaleTarget: 70
    mlflow:
      storageUri: "s3://ai-ml-platform-ml-dev-ml-artifacts/mlflow/1/artifacts/model"
      env:
      - name: MLFLOW_TRACKING_URI
        value: "http://mlflow-server.mlflow:5000"
      resources:
        requests:
          cpu: "500m"
          memory: "1Gi"
        limits:
          cpu: "1000m"
          memory: "2Gi"

---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: pytorch-cifar10
  namespace: kserve
spec:
  predictor:
    pytorch:
      storageUri: "gs://kfserving-examples/models/pytorch/cifar10"
      resources:
        requests:
          cpu: "1000m"
          memory: "1Gi"
          nvidia.com/gpu: 1
        limits:
          cpu: "2000m" 
          memory: "2Gi"
          nvidia.com/gpu: 1
  nodeSelector:
    workload: ml-serving
  tolerations:
  - key: nvidia.com/gpu
    value: "true"
    effect: NoSchedule

---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: xgboost-iris
  namespace: kserve
  annotations:
    serving.kserve.io/enable-prometheus-scraping: "true"
spec:
  predictor:
    xgboost:
      storageUri: "gs://kfserving-examples/models/xgboost/iris"
      resources:
        requests:
          cpu: "500m"
          memory: "512Mi"
        limits:
          cpu: "1000m"
          memory: "1Gi"

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: model-serving-templates
  namespace: kserve
data:
  sklearn-template.yaml: |
    apiVersion: serving.kserve.io/v1beta1
    kind: InferenceService
    metadata:
      name: "{{.ModelName}}"
      namespace: "{{.Namespace}}"
      annotations:
        serving.kserve.io/autoscaler-class: "hpa"
    spec:
      predictor:
        minReplicas: {{.MinReplicas}}
        maxReplicas: {{.MaxReplicas}}
        scaleTarget: {{.ScaleTarget}}
        sklearn:
          storageUri: "{{.StorageUri}}"
          resources:
            requests:
              cpu: "{{.CPURequest}}"
              memory: "{{.MemoryRequest}}"
            limits:
              cpu: "{{.CPULimit}}"
              memory: "{{.MemoryLimit}}"
  
  tensorflow-template.yaml: |
    apiVersion: serving.kserve.io/v1beta1
    kind: InferenceService
    metadata:
      name: "{{.ModelName}}"
      namespace: "{{.Namespace}}"
    spec:
      predictor:
        minReplicas: {{.MinReplicas}}
        maxReplicas: {{.MaxReplicas}}
        tensorflow:
          storageUri: "{{.StorageUri}}"
          resources:
            requests:
              cpu: "{{.CPURequest}}"
              memory: "{{.MemoryRequest}}"
            limits:
              cpu: "{{.CPULimit}}"
              memory: "{{.MemoryLimit}}"
  
  deployment-script.sh: |
    #!/bin/bash
    
    # KServe Model Deployment Script
    
    set -e
    
    MODEL_NAME=${1:-"my-model"}
    MODEL_TYPE=${2:-"sklearn"}
    STORAGE_URI=${3:-"s3://my-bucket/model"}
    NAMESPACE=${4:-"kserve"}
    
    echo "Deploying model: $MODEL_NAME"
    echo "Type: $MODEL_TYPE"
    echo "Storage: $STORAGE_URI"
    echo "Namespace: $NAMESPACE"
    
    # Create namespace if it doesn't exist
    kubectl create namespace $NAMESPACE --dry-run=client -o yaml | kubectl apply -f -
    
    # Deploy the model
    cat <<EOF | kubectl apply -f -
    apiVersion: serving.kserve.io/v1beta1
    kind: InferenceService
    metadata:
      name: $MODEL_NAME
      namespace: $NAMESPACE
    spec:
      predictor:
        minReplicas: 1
        maxReplicas: 3
        $MODEL_TYPE:
          storageUri: "$STORAGE_URI"
          resources:
            requests:
              cpu: "500m"
              memory: "512Mi"
            limits:
              cpu: "1000m"
              memory: "1Gi"
    EOF
    
    echo "Model deployed successfully!"
    
    # Wait for model to be ready
    echo "Waiting for model to be ready..."
    kubectl wait --for=condition=Ready inferenceservice/$MODEL_NAME -n $NAMESPACE --timeout=300s
    
    # Get the inference URL
    INFERENCE_URL=$(kubectl get inferenceservice $MODEL_NAME -n $NAMESPACE -o jsonpath='{.status.url}')
    echo "Model is ready at: $INFERENCE_URL"

---
apiVersion: v1
kind: Service
metadata:
  name: kserve-metrics
  namespace: kserve
  labels:
    app: kserve
spec:
  type: ClusterIP
  ports:
  - port: 8080
    targetPort: 8080
    name: metrics
  selector:
    control-plane: kserve-controller-manager

---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kserve-metrics
  namespace: kserve
  labels:
    app: kserve
spec:
  selector:
    matchLabels:
      app: kserve
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics