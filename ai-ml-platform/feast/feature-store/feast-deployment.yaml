apiVersion: v1
kind: Namespace
metadata:
  name: feast

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feast-config
  namespace: feast
data:
  feature_store.yaml: |
    project: ml_platform
    registry: s3://ai-ml-platform-ml-dev-ml-artifacts/feast/registry.db
    provider: aws
    online_store:
      type: redis
      connection_string: "redis://redis-service:6379"
    offline_store:
      type: file
      path: s3://ai-ml-platform-ml-dev-datasets/feast/offline
    entity_key_serialization_version: 2
    
    # AWS-specific configurations
    aws:
      region: us-west-2
      
    # Feature transformation configuration
    feature_transformation:
      mode: python
      
    # Batch ingestion configuration
    batch_engine:
      type: spark
      spark_conf:
        spark.executor.memory: "2g"
        spark.executor.cores: "2"
        spark.sql.adaptive.enabled: "true"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: feast
  labels:
    app: redis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
        command:
        - redis-server
        - --appendonly
        - "yes"
        - --save
        - "900 1"
        - --save
        - "300 10"
        - --save
        - "60 10000"
        volumeMounts:
        - name: redis-data
          mountPath: /data
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          tcpSocket:
            port: 6379
          initialDelaySeconds: 30
          timeoutSeconds: 5
        readinessProbe:
          exec:
            command:
            - redis-cli
            - ping
          initialDelaySeconds: 5
          timeoutSeconds: 1
      volumes:
      - name: redis-data
        persistentVolumeClaim:
          claimName: redis-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: redis-service
  namespace: feast
  labels:
    app: redis
spec:
  type: ClusterIP
  ports:
  - port: 6379
    targetPort: 6379
    protocol: TCP
  selector:
    app: redis

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: redis-pvc
  namespace: feast
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: gp2

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: feast-serving
  namespace: feast
  labels:
    app: feast-serving
spec:
  replicas: 2
  selector:
    matchLabels:
      app: feast-serving
  template:
    metadata:
      labels:
        app: feast-serving
    spec:
      containers:
      - name: feast-serving
        image: feastdev/feature-server:latest
        ports:
        - containerPort: 6566
          name: http
        - containerPort: 6567
          name: grpc
        env:
        - name: FEAST_USAGE
          value: "false"
        - name: AWS_DEFAULT_REGION
          value: "us-west-2"
        - name: FEAST_FEATURE_STORE_CONFIG_PATH
          value: "/feast/feature_store.yaml"
        volumeMounts:
        - name: feast-config
          mountPath: /feast
          readOnly: true
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 6566
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 6566
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: feast-config
        configMap:
          name: feast-config

---
apiVersion: v1
kind: Service
metadata:
  name: feast-serving-service
  namespace: feast
  labels:
    app: feast-serving
spec:
  type: ClusterIP
  ports:
  - port: 6566
    targetPort: 6566
    protocol: TCP
    name: http
  - port: 6567
    targetPort: 6567
    protocol: TCP
    name: grpc
  selector:
    app: feast-serving

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: feast-serving-ingress
  namespace: feast
  annotations:
    kubernetes.io/ingress.class: "nginx"
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: feast.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: feast-serving-service
            port:
              number: 6566

---
apiVersion: batch/v1
kind: Job
metadata:
  name: feast-init
  namespace: feast
spec:
  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - name: feast-init
        image: python:3.9-slim
        command:
        - /bin/bash
        - -c
        - |
          pip install feast[aws,redis]==0.32.0
          
          # Wait for Redis to be ready
          until nc -z redis-service 6379; do
            echo "Waiting for Redis..."
            sleep 5
          done
          
          python << 'EOF'
          import pandas as pd
          from feast import Entity, Feature, FeatureView, FileSource, ValueType
          from feast.feature_store import FeatureStore
          from datetime import datetime, timedelta
          import tempfile
          import os
          
          # Initialize Feast with config
          store = FeatureStore(repo_path="/feast")
          
          # Define entities
          driver_entity = Entity(
              name="driver_id",
              value_type=ValueType.INT64,
              description="Driver identifier"
          )
          
          customer_entity = Entity(
              name="customer_id", 
              value_type=ValueType.INT64,
              description="Customer identifier"
          )
          
          # Define feature sources
          driver_stats_source = FileSource(
              path="s3://ai-ml-platform-ml-dev-datasets/feast/driver_stats.parquet",
              timestamp_field="event_timestamp",
              created_timestamp_column="created"
          )
          
          customer_profile_source = FileSource(
              path="s3://ai-ml-platform-ml-dev-datasets/feast/customer_profile.parquet",
              timestamp_field="event_timestamp",
              created_timestamp_column="created"
          )
          
          # Define feature views
          driver_hourly_stats = FeatureView(
              name="driver_hourly_stats",
              entities=["driver_id"],
              ttl=timedelta(seconds=86400 * 1),
              features=[
                  Feature(name="conv_rate", dtype=ValueType.FLOAT),
                  Feature(name="acc_rate", dtype=ValueType.FLOAT),
                  Feature(name="avg_daily_trips", dtype=ValueType.INT64),
              ],
              online=True,
              batch_source=driver_stats_source,
              tags={"team": "driver_performance"},
          )
          
          customer_daily_profile = FeatureView(
              name="customer_daily_profile",
              entities=["customer_id"],
              ttl=timedelta(days=2),
              features=[
                  Feature(name="current_balance", dtype=ValueType.FLOAT),
                  Feature(name="avg_passenger_count", dtype=ValueType.FLOAT),
                  Feature(name="lifetime_trip_count", dtype=ValueType.INT64),
              ],
              online=True,
              batch_source=customer_profile_source,
              tags={"team": "customer_analytics"},
          )
          
          # Apply entities and feature views
          store.apply([driver_entity, customer_entity])
          store.apply([driver_hourly_stats, customer_daily_profile])
          
          print("Feast feature store initialized successfully!")
          
          # Create sample data
          print("Creating sample feature data...")
          
          # Driver stats sample data
          driver_data = pd.DataFrame({
              "driver_id": [1001, 1002, 1003, 1004, 1005],
              "conv_rate": [0.85, 0.90, 0.78, 0.92, 0.88],
              "acc_rate": [0.95, 0.87, 0.93, 0.89, 0.91],
              "avg_daily_trips": [25, 30, 22, 35, 28],
              "event_timestamp": [datetime.now() - timedelta(hours=i) for i in range(5)],
              "created": [datetime.now() for _ in range(5)]
          })
          
          # Customer profile sample data
          customer_data = pd.DataFrame({
              "customer_id": [2001, 2002, 2003, 2004, 2005],
              "current_balance": [125.50, 89.75, 234.10, 67.25, 178.90],
              "avg_passenger_count": [1.8, 2.2, 1.5, 2.0, 1.9],
              "lifetime_trip_count": [450, 234, 789, 123, 567],
              "event_timestamp": [datetime.now() - timedelta(hours=i) for i in range(5)],
              "created": [datetime.now() for _ in range(5)]
          })
          
          print("Sample data created successfully!")
          print(f"Driver data shape: {driver_data.shape}")
          print(f"Customer data shape: {customer_data.shape}")
          
          # In production, you would upload this data to S3
          # For now, we'll just print the data structure
          print("Driver data preview:")
          print(driver_data.head())
          print("\nCustomer data preview:")
          print(customer_data.head())
          
          EOF
        env:
        - name: AWS_DEFAULT_REGION
          value: "us-west-2"
        - name: FEAST_USAGE
          value: "false"
        volumeMounts:
        - name: feast-config
          mountPath: /feast
          readOnly: true
      volumes:
      - name: feast-config
        configMap:
          name: feast-config

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feast-scripts
  namespace: feast
data:
  materialize_features.py: |
    #!/usr/bin/env python3
    """
    Script to materialize features from offline to online store
    """
    from feast import FeatureStore
    from datetime import datetime, timedelta
    
    def materialize_features():
        """Materialize features for the last 24 hours"""
        store = FeatureStore(repo_path="/feast")
        
        # Materialize features for the last 24 hours
        end_date = datetime.now()
        start_date = end_date - timedelta(days=1)
        
        print(f"Materializing features from {start_date} to {end_date}")
        
        try:
            store.materialize(start_date, end_date)
            print("Feature materialization completed successfully!")
        except Exception as e:
            print(f"Error during materialization: {e}")
            raise
    
    if __name__ == "__main__":
        materialize_features()
  
  get_features.py: |
    #!/usr/bin/env python3
    """
    Script to retrieve features from the online store
    """
    from feast import FeatureStore
    import pandas as pd
    
    def get_features_example():
        """Example of retrieving features"""
        store = FeatureStore(repo_path="/feast")
        
        # Define entity data
        entity_df = pd.DataFrame({
            "driver_id": [1001, 1002, 1003],
            "customer_id": [2001, 2002, 2003]
        })
        
        # Get features
        features = [
            "driver_hourly_stats:conv_rate",
            "driver_hourly_stats:acc_rate", 
            "driver_hourly_stats:avg_daily_trips",
            "customer_daily_profile:current_balance",
            "customer_daily_profile:avg_passenger_count",
            "customer_daily_profile:lifetime_trip_count"
        ]
        
        try:
            feature_vector = store.get_online_features(
                features=features,
                entity_rows=entity_df.to_dict("records")
            )
            
            result_df = feature_vector.to_df()
            print("Retrieved features:")
            print(result_df)
            
            return result_df
        except Exception as e:
            print(f"Error retrieving features: {e}")
            raise
    
    if __name__ == "__main__":
        get_features_example()

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: feast-materialization
  namespace: feast
spec:
  schedule: "0 */6 * * *"  # Every 6 hours
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: materialization
            image: python:3.9-slim
            command:
            - /bin/bash
            - -c
            - |
              pip install feast[aws,redis]==0.32.0
              python /scripts/materialize_features.py
            env:
            - name: AWS_DEFAULT_REGION
              value: "us-west-2"
            - name: FEAST_USAGE
              value: "false"
            volumeMounts:
            - name: feast-config
              mountPath: /feast
              readOnly: true
            - name: scripts
              mountPath: /scripts
              readOnly: true
          volumes:
          - name: feast-config
            configMap:
              name: feast-config
          - name: scripts
            configMap:
              name: feast-scripts
              defaultMode: 0755

---
apiVersion: v1
kind: Service
metadata:
  name: feast-metrics
  namespace: feast
  labels:
    app: feast-serving
spec:
  type: ClusterIP
  ports:
  - port: 8080
    targetPort: 8080
    name: metrics
  selector:
    app: feast-serving

---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: feast-metrics
  namespace: feast
  labels:
    app: feast-serving
spec:
  selector:
    matchLabels:
      app: feast-serving
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics